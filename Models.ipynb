{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents:\n",
    "* 1 [Preparation](#intro-bullet)\n",
    "* 2 [Logistic Regression](#first-bullet)\n",
    "* 3 [Random Forest](#second-bullet)\n",
    "* 4 [Naive Bayes](#third-bullet)\n",
    "* 5 [LigthGBM](#fourth-bullet)\n",
    "    * 5.1 [Plain LigthGBM](#fifth-bullet)\n",
    "    * 5.2 [LightGBM with LR](#sixth-bullet)\n",
    "    * 5.3 [LigthGBM with RF](#seventh-bullet)\n",
    "    * 5.4 [LigthGBM with NB](#eigth-bullet)\n",
    "* 6 [XGBM](#nineth-bullet)\n",
    "    * 6.1 [Plain XGBM](#tenth-bullet)\n",
    "    * 6.2 [XGBM with LR](#eleventh-bullet)\n",
    "    * 6.3 [XGBM with RF](#12-bullet)\n",
    "    * 6.4 [XGBM with NB](#13-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation <a class=\"anchor\" id=\"intro-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_data = pd.read_csv(\"./featureData.csv\")\n",
    "# split into train test sets\n",
    "train_data, test_data = train_test_split(tmp_data,test_size=0.2)\n",
    "\n",
    "test_data.reset_index(inplace = True, drop = True)\n",
    "train_data.reset_index(inplace = True, drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=test_data.drop(columns='Unnamed: 0')\n",
    "train_data=train_data.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SK_ID_CURR</th>\n",
       "      <th>TARGET</th>\n",
       "      <th>NAME_CONTRACT_TYPE</th>\n",
       "      <th>FLAG_OWN_CAR</th>\n",
       "      <th>CNT_CHILDREN</th>\n",
       "      <th>AMT_CREDIT</th>\n",
       "      <th>AMT_ANNUITY</th>\n",
       "      <th>AMT_GOODS_PRICE</th>\n",
       "      <th>REGION_POPULATION_RELATIVE</th>\n",
       "      <th>DAYS_BIRTH</th>\n",
       "      <th>...</th>\n",
       "      <th>AMT_GOODS_PRICE_norm</th>\n",
       "      <th>DAYS_BIRTH_norm</th>\n",
       "      <th>DAYS_EMPLOYED_norm</th>\n",
       "      <th>DAYS_REGISTRATION_norm</th>\n",
       "      <th>DAYS_ID_PUBLISH_norm</th>\n",
       "      <th>OWN_CAR_AGE_norm</th>\n",
       "      <th>HOUR_APPR_PROCESS_START_norm</th>\n",
       "      <th>AMT_CREDIT_log_norm</th>\n",
       "      <th>AMT_ANNUITY_log_norm</th>\n",
       "      <th>AMT_GOODS_PRICE_log_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>177144</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>463500.0</td>\n",
       "      <td>18508.5</td>\n",
       "      <td>463500.0</td>\n",
       "      <td>0.015221</td>\n",
       "      <td>14680</td>\n",
       "      <td>...</td>\n",
       "      <td>0.105499</td>\n",
       "      <td>0.405355</td>\n",
       "      <td>0.165252</td>\n",
       "      <td>0.197147</td>\n",
       "      <td>0.633042</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>0.521739</td>\n",
       "      <td>0.518274</td>\n",
       "      <td>0.480607</td>\n",
       "      <td>0.529295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>306136</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>167895.0</td>\n",
       "      <td>16483.5</td>\n",
       "      <td>157500.0</td>\n",
       "      <td>0.020713</td>\n",
       "      <td>17992</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029181</td>\n",
       "      <td>0.592052</td>\n",
       "      <td>0.618859</td>\n",
       "      <td>0.441675</td>\n",
       "      <td>0.213700</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>0.478261</td>\n",
       "      <td>0.292605</td>\n",
       "      <td>0.457766</td>\n",
       "      <td>0.294910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>223103</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>485190.0</td>\n",
       "      <td>23472.0</td>\n",
       "      <td>405000.0</td>\n",
       "      <td>0.018801</td>\n",
       "      <td>14193</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.377903</td>\n",
       "      <td>0.319506</td>\n",
       "      <td>0.033601</td>\n",
       "      <td>0.640406</td>\n",
       "      <td>0.296703</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.528438</td>\n",
       "      <td>0.527438</td>\n",
       "      <td>0.499998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>378232</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>292500.0</td>\n",
       "      <td>14355.0</td>\n",
       "      <td>292500.0</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>9915</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062851</td>\n",
       "      <td>0.136753</td>\n",
       "      <td>0.032101</td>\n",
       "      <td>0.037168</td>\n",
       "      <td>0.360984</td>\n",
       "      <td>0.131868</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.415971</td>\n",
       "      <td>0.430513</td>\n",
       "      <td>0.429333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>418656</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>604152.0</td>\n",
       "      <td>29196.0</td>\n",
       "      <td>540000.0</td>\n",
       "      <td>0.046220</td>\n",
       "      <td>10793</td>\n",
       "      <td>...</td>\n",
       "      <td>0.124579</td>\n",
       "      <td>0.186246</td>\n",
       "      <td>0.015967</td>\n",
       "      <td>0.023752</td>\n",
       "      <td>0.252605</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>0.608696</td>\n",
       "      <td>0.577170</td>\n",
       "      <td>0.570455</td>\n",
       "      <td>0.562467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61498</th>\n",
       "      <td>174645</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>16596.0</td>\n",
       "      <td>225000.0</td>\n",
       "      <td>0.026392</td>\n",
       "      <td>20835</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046016</td>\n",
       "      <td>0.752311</td>\n",
       "      <td>0.132704</td>\n",
       "      <td>0.417599</td>\n",
       "      <td>0.577880</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.357666</td>\n",
       "      <td>0.459107</td>\n",
       "      <td>0.372361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61499</th>\n",
       "      <td>295318</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>408330.0</td>\n",
       "      <td>20979.0</td>\n",
       "      <td>292500.0</td>\n",
       "      <td>0.010147</td>\n",
       "      <td>12089</td>\n",
       "      <td>...</td>\n",
       "      <td>0.062851</td>\n",
       "      <td>0.259301</td>\n",
       "      <td>0.123604</td>\n",
       "      <td>0.082725</td>\n",
       "      <td>0.398499</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.490111</td>\n",
       "      <td>0.505304</td>\n",
       "      <td>0.429333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61500</th>\n",
       "      <td>280924</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1350000.0</td>\n",
       "      <td>39604.5</td>\n",
       "      <td>1350000.0</td>\n",
       "      <td>0.028663</td>\n",
       "      <td>11914</td>\n",
       "      <td>...</td>\n",
       "      <td>0.326599</td>\n",
       "      <td>0.249436</td>\n",
       "      <td>0.029645</td>\n",
       "      <td>0.160263</td>\n",
       "      <td>0.550368</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>0.304348</td>\n",
       "      <td>0.755853</td>\n",
       "      <td>0.630560</td>\n",
       "      <td>0.761438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61501</th>\n",
       "      <td>214690</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1061599.5</td>\n",
       "      <td>31041.0</td>\n",
       "      <td>927000.0</td>\n",
       "      <td>0.009657</td>\n",
       "      <td>10019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.221100</td>\n",
       "      <td>0.142616</td>\n",
       "      <td>0.011277</td>\n",
       "      <td>0.015686</td>\n",
       "      <td>0.370988</td>\n",
       "      <td>0.098901</td>\n",
       "      <td>0.565217</td>\n",
       "      <td>0.702444</td>\n",
       "      <td>0.582534</td>\n",
       "      <td>0.679811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61502</th>\n",
       "      <td>340268</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>889515.0</td>\n",
       "      <td>28822.5</td>\n",
       "      <td>742500.0</td>\n",
       "      <td>0.007305</td>\n",
       "      <td>14284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.175084</td>\n",
       "      <td>0.383033</td>\n",
       "      <td>0.073024</td>\n",
       "      <td>0.103518</td>\n",
       "      <td>0.623037</td>\n",
       "      <td>0.274725</td>\n",
       "      <td>0.391304</td>\n",
       "      <td>0.663141</td>\n",
       "      <td>0.567917</td>\n",
       "      <td>0.631619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61503 rows Ã— 136 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SK_ID_CURR  TARGET  NAME_CONTRACT_TYPE  FLAG_OWN_CAR  CNT_CHILDREN  \\\n",
       "0          177144       0                   0             0             0   \n",
       "1          306136       0                   0             0             0   \n",
       "2          223103       0                   0             1             0   \n",
       "3          378232       0                   0             1             0   \n",
       "4          418656       0                   0             0             0   \n",
       "...           ...     ...                 ...           ...           ...   \n",
       "61498      174645       0                   0             0             0   \n",
       "61499      295318       0                   0             0             0   \n",
       "61500      280924       0                   0             0             1   \n",
       "61501      214690       0                   0             0             2   \n",
       "61502      340268       0                   0             1             1   \n",
       "\n",
       "       AMT_CREDIT  AMT_ANNUITY  AMT_GOODS_PRICE  REGION_POPULATION_RELATIVE  \\\n",
       "0        463500.0      18508.5         463500.0                    0.015221   \n",
       "1        167895.0      16483.5         157500.0                    0.020713   \n",
       "2        485190.0      23472.0         405000.0                    0.018801   \n",
       "3        292500.0      14355.0         292500.0                    0.028663   \n",
       "4        604152.0      29196.0         540000.0                    0.046220   \n",
       "...           ...          ...              ...                         ...   \n",
       "61498    225000.0      16596.0         225000.0                    0.026392   \n",
       "61499    408330.0      20979.0         292500.0                    0.010147   \n",
       "61500   1350000.0      39604.5        1350000.0                    0.028663   \n",
       "61501   1061599.5      31041.0         927000.0                    0.009657   \n",
       "61502    889515.0      28822.5         742500.0                    0.007305   \n",
       "\n",
       "       DAYS_BIRTH  ...  AMT_GOODS_PRICE_norm  DAYS_BIRTH_norm  \\\n",
       "0           14680  ...              0.105499         0.405355   \n",
       "1           17992  ...              0.029181         0.592052   \n",
       "2           14193  ...              0.090909         0.377903   \n",
       "3            9915  ...              0.062851         0.136753   \n",
       "4           10793  ...              0.124579         0.186246   \n",
       "...           ...  ...                   ...              ...   \n",
       "61498       20835  ...              0.046016         0.752311   \n",
       "61499       12089  ...              0.062851         0.259301   \n",
       "61500       11914  ...              0.326599         0.249436   \n",
       "61501       10019  ...              0.221100         0.142616   \n",
       "61502       14284  ...              0.175084         0.383033   \n",
       "\n",
       "       DAYS_EMPLOYED_norm  DAYS_REGISTRATION_norm  DAYS_ID_PUBLISH_norm  \\\n",
       "0                0.165252                0.197147              0.633042   \n",
       "1                0.618859                0.441675              0.213700   \n",
       "2                0.319506                0.033601              0.640406   \n",
       "3                0.032101                0.037168              0.360984   \n",
       "4                0.015967                0.023752              0.252605   \n",
       "...                   ...                     ...                   ...   \n",
       "61498            0.132704                0.417599              0.577880   \n",
       "61499            0.123604                0.082725              0.398499   \n",
       "61500            0.029645                0.160263              0.550368   \n",
       "61501            0.011277                0.015686              0.370988   \n",
       "61502            0.073024                0.103518              0.623037   \n",
       "\n",
       "       OWN_CAR_AGE_norm  HOUR_APPR_PROCESS_START_norm  AMT_CREDIT_log_norm  \\\n",
       "0              0.098901                      0.521739             0.518274   \n",
       "1              0.098901                      0.478261             0.292605   \n",
       "2              0.296703                      0.347826             0.528438   \n",
       "3              0.131868                      0.608696             0.415971   \n",
       "4              0.098901                      0.608696             0.577170   \n",
       "...                 ...                           ...                  ...   \n",
       "61498          0.098901                      0.739130             0.357666   \n",
       "61499          0.098901                      0.391304             0.490111   \n",
       "61500          0.098901                      0.304348             0.755853   \n",
       "61501          0.098901                      0.565217             0.702444   \n",
       "61502          0.274725                      0.391304             0.663141   \n",
       "\n",
       "       AMT_ANNUITY_log_norm  AMT_GOODS_PRICE_log_norm  \n",
       "0                  0.480607                  0.529295  \n",
       "1                  0.457766                  0.294910  \n",
       "2                  0.527438                  0.499998  \n",
       "3                  0.430513                  0.429333  \n",
       "4                  0.570455                  0.562467  \n",
       "...                     ...                       ...  \n",
       "61498              0.459107                  0.372361  \n",
       "61499              0.505304                  0.429333  \n",
       "61500              0.630560                  0.761438  \n",
       "61501              0.582534                  0.679811  \n",
       "61502              0.567917                  0.631619  \n",
       "\n",
       "[61503 rows x 136 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression <a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "def LogRegModel(train, test):\n",
    "     # Extract the ids\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET'])\n",
    "    test= test.drop(columns = ['TARGET'])\n",
    "    # Make the model with the specified regularization parameter\n",
    "    log_reg = LogisticRegression(C = 0.0001, class_weight=dict)\n",
    "    # Train on the training data\n",
    "    log_reg.fit(train, labels)\n",
    "    # Select only second column(TARGET)\n",
    "    log_reg_pred = log_reg.predict_proba(test)[:, 1]\n",
    "    tescik = log_reg.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    \n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, log_reg_pred))\n",
    "    print('F1 score: %f' % f1)\n",
    "    return roc_auc_score(test_labels, log_reg_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test split results:\n",
      "ROC 0.6253816156491754\n",
      "F1 score: 0.000000\n"
     ]
    }
   ],
   "source": [
    "LogRegScore = LogRegModel(train_data,test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest <a class=\"anchor\" id=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def RanForModel(train,test):\n",
    "    rf = RandomForestClassifier(n_estimators=100,\n",
    "                                max_depth=10,min_samples_split=20,\n",
    "                                min_samples_leaf=6,\n",
    "                                max_features='auto')\n",
    "     # Extract the ids\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET'])\n",
    "    test= test.drop(columns = ['TARGET'])\n",
    "    \n",
    "    rf.fit(X = train, y = labels)\n",
    "    # Select only second column(TARGET)\n",
    "    ran_for_pred = rf.predict_proba(test)[:, 1]\n",
    "    \n",
    "    tescik = rf.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    print('F1 score: %f' % f1)\n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, ran_for_pred))\n",
    "    return roc_auc_score(test_labels, ran_for_pred)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.000000\n",
      "Train/Test split results:\n",
      "ROC 0.736639010430749\n"
     ]
    }
   ],
   "source": [
    "RandForScore = RanForModel(train_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "def imbalancedRanFor(train,test):\n",
    "    rf = BalancedRandomForestClassifier()\n",
    "     # Extract the ids\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET'])\n",
    "    test= test.drop(columns = ['TARGET'])\n",
    "    \n",
    "    rf.fit(X = train, y = labels)\n",
    "    # Select only second column(TARGET)\n",
    "    ran_for_pred = rf.predict_proba(test)[:, 1]\n",
    "    \n",
    "    tescik = rf.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    print('F1 score: %f' % f1)\n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, ran_for_pred))\n",
    "    return roc_auc_score(test_labels, ran_for_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.256361\n",
      "Train/Test split results:\n",
      "ROC 0.7383087691968435\n"
     ]
    }
   ],
   "source": [
    "imbRanFor =imbalancedRanFor(train_data,test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Naive Bayesn <a class=\"anchor\" id=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def NaiveBayModel(train,test):\n",
    "    \n",
    "    \n",
    "    clf = BernoulliNB()\n",
    "    \n",
    "     # Extract the ids\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "#     train = train[[\"REGION_RATING_CLIENT_W_CITY\",\"REGION_RATING_CLIENT\",\"NAME_INCOME_TYPE_Working\",\n",
    "#                   \"DAYS_LAST_PHONE_CHANGE\",\"CODE_GENDER_M\",\"NAME_EDUCATION_TYPE_Secondary / secondary special\",\n",
    "#                   \"REG_CITY_NOT_WORK_CITY\",\"FLAG_EMP_PHONE\",\"REG_CITY_NOT_LIVE_CITY\",\"EXT_SOURCE_2\",\n",
    "#                   \"EXT_SOURCE_3\",\"EXT_SOURCE_1\",\"DAYS_BIRTH\",\"DAYS_EMPLOYED\",\n",
    "#                   \"NAME_EDUCATION_TYPE_Higher education\",\"CODE_GENDER_F\",\"DAYS_ID_PUBLISH\",\n",
    "#                   \"NAME_INCOME_TYPE_Pensioner\",\"ORGANIZATION_TYPE_XNA\"]]\n",
    "#     test = test[[\"REGION_RATING_CLIENT_W_CITY\",\"REGION_RATING_CLIENT\",\"NAME_INCOME_TYPE_Working\",\n",
    "#                   \"DAYS_LAST_PHONE_CHANGE\",\"CODE_GENDER_M\",\"NAME_EDUCATION_TYPE_Secondary / secondary special\",\n",
    "#                   \"REG_CITY_NOT_WORK_CITY\",\"FLAG_EMP_PHONE\",\"REG_CITY_NOT_LIVE_CITY\",\"EXT_SOURCE_2\",\n",
    "#                   \"EXT_SOURCE_3\",\"EXT_SOURCE_1\",\"DAYS_BIRTH\",\"DAYS_EMPLOYED\",\n",
    "#                   \"NAME_EDUCATION_TYPE_Higher education\",\"CODE_GENDER_F\",\"DAYS_ID_PUBLISH\",\n",
    "#                   \"NAME_INCOME_TYPE_Pensioner\",\"ORGANIZATION_TYPE_XNA\"]]\n",
    "    clf.fit(X = train, y = labels)\n",
    "    clf_pred = clf.predict_proba(test)[:, 1]\n",
    "    tescik = clf.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    print('F1 score: %f' % f1)\n",
    "\n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, clf_pred))\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    print('F1 score: %f' % f1)\n",
    "    return roc_auc_score(test_labels, clf_pred)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.152971\n",
      "Train/Test split results:\n",
      "ROC 0.6261607516294776\n",
      "F1 score: 0.152971\n"
     ]
    }
   ],
   "source": [
    "NaiveScore = NaiveBayModel(train_data,test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. LigthGBM <a class=\"anchor\" id=\"fourth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Plain LightGBM <a class=\"anchor\" id=\"fifth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 LightGBM with LR <a class=\"anchor\" id=\"sixth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "def PlainLightGBM(train,test):\n",
    "    \n",
    "    \n",
    "    \n",
    "     # Extract the ids\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "   \n",
    "    model = lgb.LGBMClassifier()\n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "#     train = train[[\"REGION_RATING_CLIENT_W_CITY\",\"REGION_RATING_CLIENT\",\"NAME_INCOME_TYPE_Working\",\n",
    "#                   \"DAYS_LAST_PHONE_CHANGE\",\"CODE_GENDER_M\",\"NAME_EDUCATION_TYPE_Secondary / secondary special\",\n",
    "#                   \"REG_CITY_NOT_WORK_CITY\",\"FLAG_EMP_PHONE\",\"REG_CITY_NOT_LIVE_CITY\",\"EXT_SOURCE_2\",\n",
    "#                   \"EXT_SOURCE_3\",\"EXT_SOURCE_1\",\"DAYS_BIRTH\",\"DAYS_EMPLOYED\",\n",
    "#                   \"NAME_EDUCATION_TYPE_Higher education\",\"CODE_GENDER_F\",\"DAYS_ID_PUBLISH\",\n",
    "#                   \"NAME_INCOME_TYPE_Pensioner\",\"ORGANIZATION_TYPE_XNA\"]]\n",
    "#     test = test[[\"REGION_RATING_CLIENT_W_CITY\",\"REGION_RATING_CLIENT\",\"NAME_INCOME_TYPE_Working\",\n",
    "#                   \"DAYS_LAST_PHONE_CHANGE\",\"CODE_GENDER_M\",\"NAME_EDUCATION_TYPE_Secondary / secondary special\",\n",
    "#                   \"REG_CITY_NOT_WORK_CITY\",\"FLAG_EMP_PHONE\",\"REG_CITY_NOT_LIVE_CITY\",\"EXT_SOURCE_2\",\n",
    "#                   \"EXT_SOURCE_3\",\"EXT_SOURCE_1\",\"DAYS_BIRTH\",\"DAYS_EMPLOYED\",\n",
    "#                   \"NAME_EDUCATION_TYPE_Higher education\",\"CODE_GENDER_F\",\"DAYS_ID_PUBLISH\",\n",
    "#                   \"NAME_INCOME_TYPE_Pensioner\",\"ORGANIZATION_TYPE_XNA\"]]\n",
    "    train = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    test = test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=1)\n",
    "    n_scores = cross_val_score(model, train, labels, scoring='roc_auc_ovo_weighted', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "    model.fit(train, labels)\n",
    "    model_pred = model.predict_proba(test)[:, 1]\n",
    "   \n",
    "    tescik = model.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "     \n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, model_pred))\n",
    "    print('F1 score: %f' % f1)\n",
    "    cm = confusion_matrix(test_labels, tescik)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()\n",
    "    return roc_auc_score(test_labels, model_pred)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test split results:\n",
      "ROC 0.7644163583522577\n",
      "F1 score: 0.040250\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.7644163583522577"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plainLGBMScore=PlainLightGBM(train_data,test_data)\n",
    "PlainLightGBM(train_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 LightGBM with RF <a class=\"anchor\" id=\"seventh-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "def multiplePer(train, test):\n",
    "     # Extract the ids\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    \n",
    "    clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    clf.fit(train, labels)\n",
    "    model_pred = clf.predict_proba(test)[:, 1]\n",
    "   \n",
    "    tescik = clf.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "     \n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, model_pred))\n",
    "    print('F1 score: %f' % f1)\n",
    "    cm = confusion_matrix(test_labels, tescik)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test split results:\n",
      "ROC 0.500114924238406\n",
      "F1 score: 0.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "multiplePer(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 LightGBM with NB <a class=\"anchor\" id=\"eigth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. XGBM <a class=\"anchor\" id=\"nineth-bullet\"></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Plain XGBM -- to long do not use <a class=\"anchor\" id=\"tenth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import re\n",
    "\n",
    "# def PlainXGBM(train,test):\n",
    "    \n",
    "#     model = XGBClassifier()\n",
    "#      # Extract the ids\n",
    "#     train_ids = train['SK_ID_CURR']\n",
    "#     test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "#     # Extract the labels for training\n",
    "#     labels = train['TARGET']\n",
    "#     test_labels = test['TARGET']\n",
    "#     # Remove the ids and target\n",
    "#     train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "#     test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "#     train = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "#     model.fit(X = train, y = labels)\n",
    "#     model_pred = model.predict_proba(test)[:, 1]\n",
    "    \n",
    "    \n",
    "    \n",
    "#     print('Train/Test split results:')\n",
    "#     print(\"ROC\",  roc_auc_score(test_labels, model_pred))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PlainXGBM(train_data,test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 XGBM with LR<a class=\"anchor\" id=\"eleventh-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 XGBM with RF <a class=\"anchor\" id=\"12-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 XGBM with NB <a class=\"anchor\" id=\"13-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not completed -- do not use it \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def random_search(param_grid, max_evals = 5):\n",
    "#     \"\"\"Random search for hyperparameter optimization\"\"\"\n",
    "    \n",
    "#     # Dataframe for results\n",
    "#     results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "#                                   index = list(range(5)))\n",
    "    \n",
    "#     # Keep searching until reach max evaluations\n",
    "#     for i in range(5):\n",
    "        \n",
    "#         # Choose random hyperparameters\n",
    "#         hyperparameters = {k: random.sample(v, 1)[0] for k, v in param_grid.items()}\n",
    "#         hyperparameters['subsample'] = 1.0 if hyperparameters['boosting_type'] == 'goss' else hyperparameters['subsample']\n",
    "\n",
    "#         # Evaluate randomly selected hyperparameters\n",
    "#         eval_results = objective(hyperparameters, i)\n",
    "        \n",
    "#         results.loc[i, :] = eval_results\n",
    "    \n",
    "#     # Sort with best score on top\n",
    "#     results.sort_values('score', ascending = False, inplace = True)\n",
    "#     results.reset_index(inplace = True)\n",
    "#     return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def newModel(train,test):\n",
    "#     # Extract the test ids and train labels\n",
    "#     test_ids = test['SK_ID_CURR']\n",
    "#     train_labels = np.array(train['TARGET'].astype(np.int32)).reshape((-1, ))\n",
    "\n",
    "#     train = train.drop(columns = ['SK_ID_CURR', 'TARGET'])\n",
    "#     test = test.drop(columns = ['SK_ID_CURR'])\n",
    "\n",
    "#     print('Training shape: ', train.shape)\n",
    "#     print('Testing shape: ', test.shape)\n",
    "#     train_set = lgb.Dataset(train, label = train_labels)\n",
    "#     random_results = pd.DataFrame(columns = ['score', 'params', 'iteration'],\n",
    "#                               index = list(range(5)))\n",
    "#     random_search_params = random_results.loc[0, 'params']\n",
    "    \n",
    "#     hyperparameters = dict(**random_results.loc[0, 'hyperparameters'])\n",
    "#     del hyperparameters['n_estimators']\n",
    "\n",
    "#     # Cross validation with n_folds and early stopping\n",
    "#     cv_results = lgb.cv(hyperparameters, train_set,\n",
    "#                     num_boost_round = 10000, early_stopping_rounds = 100, \n",
    "#                     metrics = 'auc', nfold = N_FOLDS)\n",
    "    \n",
    "#     print('The cross validation score on the full dataset = {:.5f} with std: {:.5f}.'.format(\n",
    "#     cv_results['auc-mean'][-1], cv_results['auc-stdv'][-1]))\n",
    "#     print('Number of estimators = {}.'.format(len(cv_results['auc-mean'])))\n",
    "#     # Train the model with the optimal number of estimators from early stopping\n",
    "#     model = lgb.LGBMClassifier(n_estimators = len(cv_results['auc-mean']), **hyperparameters)\n",
    "#     model.fit(train, train_labels)\n",
    "                        \n",
    "#     # Predictions on the test data\n",
    "#     preds = model.predict_proba(test)[:, 1]\n",
    "#     submission = pd.DataFrame({'SK_ID_CURR': test_ids, 'TARGET': preds})\n",
    "#     submission.to_csv('submission_simple_features_random.csv', index = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newModel(train_data,test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not complete part 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # compare ensemble to each baseline classifier\n",
    "# from numpy import mean\n",
    "# from numpy import std\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.ensemble import StackingClassifier\n",
    "# from matplotlib import pyplot\n",
    " \n",
    "# def get_stacking():\n",
    "#     # define the base models\n",
    "#     level0 = list()\n",
    "#     level0.append(('lr', LogisticRegression()))\n",
    "#     level0.append(('knn', KNeighborsClassifier()))\n",
    "#     level0.append(('cart', DecisionTreeClassifier()))\n",
    "#     level0.append(('svm', SVC()))\n",
    "#     # define meta learner model\n",
    "#     level1 = lgb.LGBMClassifier()\n",
    "#     # define the stacking ensemble\n",
    "#     model = StackingClassifier(estimators=level0, final_estimator=level1, cv=2)\n",
    "#     return model\n",
    "\n",
    " \n",
    "# # get a list of models to evaluate\n",
    "# def get_models():\n",
    "#     models = dict()\n",
    "# #     models['lr'] = LogisticRegression()\n",
    "# #     models['knn'] = KNeighborsClassifier()\n",
    "# #     models['cart'] = DecisionTreeClassifier()\n",
    "# #     models['svm'] = SVC()\n",
    "# #     models['bayes'] = GaussianNB()\n",
    "#     models['stacking'] = get_stacking()\n",
    "#     return models\n",
    " \n",
    "# # evaluate a give model using cross-validation\n",
    "# def evaluate_model(model, X, y):\n",
    "#     cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "#     scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1, error_score='raise')\n",
    "#     return scores\n",
    " \n",
    "\n",
    "# def extraFun(train, test):\n",
    "#     # define dataset\n",
    "#     # Extract the ids\n",
    "#     train_ids = train['SK_ID_CURR']\n",
    "#     test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "#     # Extract the labels for training\n",
    "#     labels = train['TARGET']\n",
    "#     test_labels = test['TARGET']\n",
    "#     # Remove the ids and target\n",
    "#     train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "#     test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "#     train = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "#     X = train\n",
    "#     y = labels\n",
    "#     # get the models to evaluate\n",
    "#     models = get_models()\n",
    "#     # evaluate the models and store results\n",
    "#     results, names = list(), list()\n",
    "#     for name, model in models.items():\n",
    "#         scores = evaluate_model(model, X, y)\n",
    "#         results.append(scores)\n",
    "#         names.append(name)\n",
    "#         print('>%s %.3f (%.3f)' % (name, mean(scores), std(scores)))\n",
    "#     # plot model performance for comparison\n",
    "#     pyplot.boxplot(results, labels=names, showmeans=True)\n",
    "#     pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraFun(train_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    " # compare ensemble to each baseline classifier\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from matplotlib import pyplot\n",
    "def fun2(train, test):\n",
    "    # define dataset\n",
    "    # define dataset\n",
    "    # Extract the ids\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "#     train = train[[\"REGION_RATING_CLIENT_W_CITY\",\"REGION_RATING_CLIENT\",\"NAME_INCOME_TYPE_Working\",\n",
    "#                   \"DAYS_LAST_PHONE_CHANGE\",\"CODE_GENDER_M\",\"NAME_EDUCATION_TYPE_Secondary / secondary special\",\n",
    "#                   \"REG_CITY_NOT_WORK_CITY\",\"FLAG_EMP_PHONE\",\"REG_CITY_NOT_LIVE_CITY\",\"EXT_SOURCE_2\",\n",
    "#                   \"EXT_SOURCE_3\",\"EXT_SOURCE_1\",\"DAYS_BIRTH\",\"DAYS_EMPLOYED\",\n",
    "#                   \"NAME_EDUCATION_TYPE_Higher education\",\"CODE_GENDER_F\",\"DAYS_ID_PUBLISH\",\n",
    "#                   \"NAME_INCOME_TYPE_Pensioner\",\"ORGANIZATION_TYPE_XNA\"]]\n",
    "#     test = test[[\"REGION_RATING_CLIENT_W_CITY\",\"REGION_RATING_CLIENT\",\"NAME_INCOME_TYPE_Working\",\n",
    "#                   \"DAYS_LAST_PHONE_CHANGE\",\"CODE_GENDER_M\",\"NAME_EDUCATION_TYPE_Secondary / secondary special\",\n",
    "#                   \"REG_CITY_NOT_WORK_CITY\",\"FLAG_EMP_PHONE\",\"REG_CITY_NOT_LIVE_CITY\",\"EXT_SOURCE_2\",\n",
    "#                   \"EXT_SOURCE_3\",\"EXT_SOURCE_1\",\"DAYS_BIRTH\",\"DAYS_EMPLOYED\",\n",
    "#                   \"NAME_EDUCATION_TYPE_Higher education\",\"CODE_GENDER_F\",\"DAYS_ID_PUBLISH\",\n",
    "#                   \"NAME_INCOME_TYPE_Pensioner\",\"ORGANIZATION_TYPE_XNA\"]]\n",
    "    train = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    X = train\n",
    "    y = labels\n",
    "    # define the base models\n",
    "    level0 = list()\n",
    "    level0.append(('rf',RandomForestClassifier(n_estimators=100,\n",
    "                                max_depth=10,min_samples_split=20,\n",
    "                                min_samples_leaf=6,\n",
    "                                max_features='auto')))\n",
    "    level0.append(('bayes', BernoulliNB()))\n",
    "    # define meta learner model\n",
    "    level1 = LogisticRegression(C = 0.0001)\n",
    "    # define the stacking ensemble\n",
    "    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=2)\n",
    "    # fit the model on all available data\n",
    "    model.fit(X, y)\n",
    "    # make a prediction for one example\n",
    "    model_pred = model.predict_proba(test)[:,1]\n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, model_pred))\n",
    "    return roc_auc_score(test_labels, model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test split results:\n",
      "ROC 0.6930841135868887\n"
     ]
    }
   ],
   "source": [
    "StackingScore = fun2(train_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      TOP 10 Positive Correlations:\n",
      " TARGET                                               1.000000\n",
      "REGION_RATING_CLIENT_W_CITY                          0.060586\n",
      "REGION_RATING_CLIENT                                 0.058288\n",
      "NAME_INCOME_TYPE_Working                             0.057026\n",
      "DAYS_LAST_PHONE_CHANGE                               0.055802\n",
      "CODE_GENDER_M                                        0.054864\n",
      "NAME_EDUCATION_TYPE_Secondary / secondary special    0.050017\n",
      "REG_CITY_NOT_WORK_CITY                               0.049181\n",
      "FLAG_EMP_PHONE                                       0.045471\n",
      "FLAG_DOCUMENT_3                                      0.044827\n",
      "REG_CITY_NOT_LIVE_CITY                               0.044250\n",
      "Name: TARGET, dtype: float64\n",
      "\n",
      "         TOP 10 Negative Correlations:\n",
      " EXT_SOURCE_2                           -0.160947\n",
      "EXT_SOURCE_3                           -0.155876\n",
      "EXT_SOURCE_1                           -0.099254\n",
      "DAYS_BIRTH                             -0.076343\n",
      "DAYS_EMPLOYED                          -0.063845\n",
      "NAME_EDUCATION_TYPE_Higher education   -0.056318\n",
      "CODE_GENDER_F                          -0.054856\n",
      "DAYS_ID_PUBLISH                        -0.050808\n",
      "NAME_INCOME_TYPE_Pensioner             -0.045732\n",
      "ORGANIZATION_TYPE_XNA                  -0.045452\n",
      "Name: TARGET, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "1         0\n",
       "2         1\n",
       "3         0\n",
       "4         1\n",
       "         ..\n",
       "246003    0\n",
       "246004    0\n",
       "246005    1\n",
       "246006    0\n",
       "246007    1\n",
       "Name: NAME_EDUCATION_TYPE_Secondary / secondary special, Length: 246008, dtype: int64"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[\"NAME_EDUCATION_TYPE_Secondary / secondary special\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Series' object has no attribute 'column'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-ddac3c232ad4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlast_ten\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5273\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5276\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Series' object has no attribute 'column'"
     ]
    }
   ],
   "source": [
    "last_ten.column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# make predictions using gradient boosting for classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import re\n",
    "\n",
    "def myBooster(train,test):\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    train = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    # define dataset\n",
    "    X = train\n",
    "    y = labels\n",
    "    # define the model\n",
    "    model = GradientBoostingClassifier()\n",
    "    cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=1)\n",
    "    n_scores = cross_val_score(model, X, y, scoring='roc_auc_ovo_weighted', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    model.fit(X, y)\n",
    "    # make a prediction for one example\n",
    "    model_pred = model.predict_proba(test)[:,1]\n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, model_pred))\n",
    "    return roc_auc_score(test_labels, model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test split results:\n",
      "ROC 0.7383597630786167\n"
     ]
    }
   ],
   "source": [
    "GradBoostScore = myBooster(train_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "\n",
    "sklearn.metrics.SCORERS.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "def myHistBooster(train, test):\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    train = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    # define dataset\n",
    "    X = train\n",
    "    y = labels\n",
    "    model = HistGradientBoostingClassifier()\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "    n_scores = cross_val_score(model, X, y, scoring='roc_auc_ovo_weighted', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    model_pred = model.predict_proba(test)[:,1]\n",
    "    tescik = model.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, model_pred))\n",
    "    print('F1 score: %f' % f1)\n",
    "    return roc_auc_score(test_labels, model_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train/Test split results:\n",
      "ROC 0.7385603160320114\n",
      "F1 score: 0.019208\n"
     ]
    }
   ],
   "source": [
    "Hist1BoosterScore = myHistBooster(train_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.experimental import enable_hist_gradient_boosting\n",
    "# from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "# from sklearn.datasets import make_classification\n",
    "# from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.ensemble import GradientBoostingClassifier\n",
    "# def myHistBooster2(train, test):\n",
    "#     train_ids = train['SK_ID_CURR']\n",
    "#     test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "#     # Extract the labels for training\n",
    "#     labels = train['TARGET']\n",
    "#     test_labels = test['TARGET']\n",
    "#     # Remove the ids and target\n",
    "#     train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "#     test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "#     train = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "#     # define dataset\n",
    "#     X = train\n",
    "#     y = labels\n",
    "#     model = HistGradientBoostingClassifier()\n",
    "#     grid = dict()\n",
    "#     grid['learning_rate'] = [0.01]\n",
    "    \n",
    "#     grid['max_depth'] = [3]\n",
    "#     # define the evaluation procedure\n",
    "#     cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "#     # define the grid search procedure\n",
    "#     grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='roc_auc_ovo_weighted',error_score='raise')\n",
    "#     # execute the grid search\n",
    "#     grid_result = grid_search.fit(X, y)\n",
    "#     # summarize the best score and configuration\n",
    "#     print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "#     # summarize all scores that were evaluated\n",
    "#     means = grid_result.cv_results_['mean_test_score']\n",
    "#     stds = grid_result.cv_results_['std_test_score']\n",
    "#     params = grid_result.cv_results_['params']\n",
    "#     for mean, stdev, param in zip(means, stds, params):\n",
    "#         print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hist2BoosterScore = myHistBooster2(train_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "from sklearn import model_selection \n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "def bagging(train, test):\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "#     train = train[[\"REGION_RATING_CLIENT_W_CITY\",\"REGION_RATING_CLIENT\",\"NAME_INCOME_TYPE_Working\",\n",
    "#                   \"DAYS_LAST_PHONE_CHANGE\",\"CODE_GENDER_M\",\"NAME_EDUCATION_TYPE_Secondary / secondary special\",\n",
    "#                   \"REG_CITY_NOT_WORK_CITY\",\"FLAG_EMP_PHONE\",\"REG_CITY_NOT_LIVE_CITY\",\"EXT_SOURCE_2\",\n",
    "#                   \"EXT_SOURCE_3\",\"EXT_SOURCE_1\",\"DAYS_BIRTH\",\"DAYS_EMPLOYED\",\n",
    "#                   \"NAME_EDUCATION_TYPE_Higher education\",\"CODE_GENDER_F\",\"DAYS_ID_PUBLISH\",\n",
    "#                   \"NAME_INCOME_TYPE_Pensioner\",\"ORGANIZATION_TYPE_XNA\"]]\n",
    "#     test = test[[\"REGION_RATING_CLIENT_W_CITY\",\"REGION_RATING_CLIENT\",\"NAME_INCOME_TYPE_Working\",\n",
    "#                   \"DAYS_LAST_PHONE_CHANGE\",\"CODE_GENDER_M\",\"NAME_EDUCATION_TYPE_Secondary / secondary special\",\n",
    "#                   \"REG_CITY_NOT_WORK_CITY\",\"FLAG_EMP_PHONE\",\"REG_CITY_NOT_LIVE_CITY\",\"EXT_SOURCE_2\",\n",
    "#                   \"EXT_SOURCE_3\",\"EXT_SOURCE_1\",\"DAYS_BIRTH\",\"DAYS_EMPLOYED\",\n",
    "#                   \"NAME_EDUCATION_TYPE_Higher education\",\"CODE_GENDER_F\",\"DAYS_ID_PUBLISH\",\n",
    "#                   \"NAME_INCOME_TYPE_Pensioner\",\"ORGANIZATION_TYPE_XNA\"]]\n",
    "    train = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    \n",
    "    # define dataset\n",
    "    X = train\n",
    "    y = labels\n",
    "    seed = 1\n",
    "   \n",
    "    # initialize the base classifier \n",
    "    base_cls = RandomForestClassifier(n_estimators = 100, n_jobs = -1)\n",
    "     \n",
    "  \n",
    "    # no. of base classifier \n",
    "    num_trees = 100\n",
    "  \n",
    "    # bagging classifier \n",
    "    model = BaggingClassifier(base_estimator = base_cls, \n",
    "                          random_state = seed,\n",
    "                             n_jobs=-1) \n",
    "    print(\"hello\")\n",
    "    model.fit(X,y)\n",
    "    model_pred = model.predict_proba(test)[:,1]\n",
    "    tescik = model.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, model_pred))\n",
    "    print('F1 score: %f' % f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n",
      "Train/Test split results:\n",
      "ROC 0.7210763465942981\n",
      "F1 score: 0.005154\n"
     ]
    }
   ],
   "source": [
    "BagginScore=bagging(train_data,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents:\n",
    "* 1 [Preparation](#intro-bullet)\n",
    "* 2 [Logistic Regression](#first-bullet)\n",
    "* 3 [Random Forest](#second-bullet)\n",
    "* 3 [Balanced Random Forest](#third-bullet)\n",
    "* 4 [Naive Bayes](#fourth-bullet)\n",
    "* 5 [KNN](#fifth-bullet)\n",
    "* 6 [Decision Tree](#sixth-bullet)\n",
    "* 7 [Stacking](#seventh-bullet)\n",
    "* 8 [Boosting](#eigth-bullet)\n",
    "    * 8.1 [LigthGBM](#nineth-bullet)\n",
    "    * 8.2 [Gradient Boosting](#tenth-bullet)\n",
    "    * 8.3 [Histogram Gradient Boosting 1](#11-bullet)\n",
    "* 9 [Bagging](#12-bullet)\n",
    "* 10 [Plotting](#13-bullet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparation <a class=\"anchor\" id=\"intro-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numpy as mean\n",
    "import numpy as std\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, roc_auc_score, confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMARK\n",
    "the clf/model/rf or other names assigned to names of classifiers are estimator instances. There are firsty fitted to the model; that is, it must learn from the model. This is done by passing our training set to the **fit** method. Then we use **predic** or **predict_proba** to predict values of TARGET column or probability of such value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic feature engineering\n",
    "tmp_data1 = pd.read_csv(\"./featureData1.csv\")\n",
    "\n",
    "# split into train test sets\n",
    "train_data1, test_data1 = train_test_split(tmp_data1,test_size=0.2)\n",
    "test_data1.reset_index(inplace = True, drop = True)\n",
    "train_data1.reset_index(inplace = True, drop = True)\n",
    "\n",
    "# More detailed feature engineering\n",
    "tmp_data2 = pd.read_csv(\"./featureData2.csv\")\n",
    "\n",
    "# split into train test sets\n",
    "train_data2, test_data2 = train_test_split(tmp_data2,test_size=0.2)\n",
    "test_data2.reset_index(inplace = True, drop = True)\n",
    "train_data2.reset_index(inplace = True, drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns \"Unnamed: 0\"\n",
    "test_data1=test_data1.drop(columns='Unnamed: 0')\n",
    "train_data1=train_data1.drop(columns='Unnamed: 0')\n",
    "\n",
    "test_data2=test_data2.drop(columns='Unnamed: 0')\n",
    "train_data2=train_data2.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression <a class=\"anchor\" id=\"first-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "def LogRegModel(train, test):\n",
    "     # Extract the ids\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET'])\n",
    "    test= test.drop(columns = ['TARGET'])\n",
    "    \n",
    "    # Make the model with the specified regularization parameter\n",
    "    # log_reg = LogisticRegression(C = 0.0001, class_weight=\"balanced\")\n",
    "    weights = {0:0.0878, 1:0.9122}\n",
    "    log_reg = LogisticRegression(C = 0.0001, class_weight=weights)\n",
    "\n",
    "    # Train on the training data\n",
    "    log_reg.fit(train, labels)\n",
    "    # Select only second column(TARGET)\n",
    "    log_reg_pred = log_reg.predict_proba(test)[:, 1]\n",
    "    tescik = log_reg.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    cm = confusion_matrix(test_labels, tescik)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, log_reg_pred))\n",
    "    print('F1 score: %f' % f1)\n",
    "    return roc_auc_score(test_labels, log_reg_pred), log_reg_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LRScore1 = LogRegModel(train_data1,test_data1)\n",
    "LRScore2 = LogRegModel(train_data2,test_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest <a class=\"anchor\" id=\"second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def RanForModel(train,test):\n",
    "    rf = RandomForestClassifier(n_estimators=100,\n",
    "                                max_depth=10,min_samples_split=20,\n",
    "                                min_samples_leaf=6,\n",
    "                                max_features='auto')\n",
    "     # Extract the ids\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET'])\n",
    "    test= test.drop(columns = ['TARGET'])\n",
    "    \n",
    "    rf.fit(X = train, y = labels)\n",
    "    # Select only second column(TARGET)\n",
    "    ran_for_pred = rf.predict_proba(test)[:, 1]\n",
    "    \n",
    "    tescik = rf.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    cm = confusion_matrix(test_labels, tescik)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    print('F1 score: %f' % f1)\n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, ran_for_pred))\n",
    "    return roc_auc_score(test_labels, ran_for_pred),ran_for_pred\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RFScore1 = RanForModel(train_data1,test_data1)\n",
    "RFScore2 = RanForModel(train_data2,test_data2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Balanced Random Forest <a class=\"anchor\" id=\"third-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.ensemble import BalancedRandomForestClassifier\n",
    "\n",
    "def imbalancedRanFor(train,test):\n",
    "    rf = BalancedRandomForestClassifier(class_weight='balanced')\n",
    "     # Extract the ids\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET'])\n",
    "    test= test.drop(columns = ['TARGET'])\n",
    "    \n",
    "    rf.fit(X = train, y = labels)\n",
    "    # Select only second column(TARGET)\n",
    "    ran_for_pred = rf.predict_proba(test)[:, 1]\n",
    "    \n",
    "    tescik = rf.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    cm = confusion_matrix(test_labels, tescik)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    print('F1 score: %f' % f1)\n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, ran_for_pred))\n",
    "    \n",
    "    return roc_auc_score(test_labels, ran_for_pred),ran_for_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imbRFScore1 =imbalancedRanFor(train_data1,test_data1)\n",
    "imbRFScore2 =imbalancedRanFor(train_data2,test_data2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Naive Bayesian<a class=\"anchor\" id=\"fourth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def NaiveBayModel(train,test):\n",
    "    \n",
    "    \n",
    "    clf = BernoulliNB()\n",
    "    \n",
    "     # Extract the ids\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    clf.fit(X = train, y = labels)\n",
    "    clf_pred = clf.predict_proba(test)[:, 1]\n",
    "    tescik = clf.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    cm = confusion_matrix(test_labels, tescik)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, clf_pred))\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    print('F1 score: %f' % f1)\n",
    "    return roc_auc_score(test_labels, clf_pred),clf_pred\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NBScore1 = NaiveBayModel(train_data1,test_data1)\n",
    "NBScore2 = NaiveBayModel(train_data2,test_data2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. K-Nearest Neighbors <a class=\"anchor\" id=\"fifth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def KNNModel(train,test):\n",
    "    \n",
    "    \n",
    "    clf = KNeighborsClassifier(n_neighbors=3)\n",
    "    \n",
    "     # Extract the ids\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    clf.fit(X = train, y = labels)\n",
    "    clf_pred = clf.predict_proba(test)[:, 1]\n",
    "    tescik = clf.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    cm = confusion_matrix(test_labels, tescik)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, clf_pred))\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    print('F1 score: %f' % f1)\n",
    "    return roc_auc_score(test_labels, clf_pred), clf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNNScore1 = KNNModel(train_data1,test_data1)\n",
    "KNNScore2 = KNNModel(train_data2,test_data2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Decision Tree <a class=\"anchor\" id=\"sixth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "def DTModel(train,test):\n",
    "    \n",
    "    weights = {0:0.0878, 1:0.9122}\n",
    "   \n",
    "    clf = DecisionTreeClassifier( class_weight=weights)\n",
    "    \n",
    "     # Extract the ids\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    clf.fit(X = train, y = labels)\n",
    "    clf_pred = clf.predict_proba(test)[:, 1]\n",
    "    tescik = clf.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    cm = confusion_matrix(test_labels, tescik)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, clf_pred))\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    print('F1 score: %f' % f1)\n",
    "    \n",
    "    return roc_auc_score(test_labels, clf_pred),clf_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DTScore1 = DTModel(train_data1,test_data1)\n",
    "DTScore2 = DTModel(train_data2,test_data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import mean\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "def extra(train,test):\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    train = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    \n",
    "    # define dataset\n",
    "    X = train\n",
    "    y = labels\n",
    "    # define pipeline\n",
    "    steps = [('under', RandomUnderSampler()), ('model', DecisionTreeClassifier())]\n",
    "    pipeline = Pipeline(steps=steps)\n",
    "    # evaluate pipeline\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "    scores = cross_val_score(pipeline, X, y, scoring='f1_micro', cv=cv, n_jobs=-1)\n",
    "    score = mean(scores)\n",
    "    print('F1 Score: %.3f' % score)\n",
    "    pipeline.fit(X,y)\n",
    "    model_pred = pipeline.predict_proba(test)[:,1]\n",
    "    tescik = pipeline.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    cm = confusion_matrix(test_labels, tescik)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, model_pred)), model_pred\n",
    "    print('F1 score: %f' % f1)\n",
    "    \n",
    "    return roc_auc_score(test_labels, model_pred), model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra1 = extra(train_data1,test_data1)\n",
    "extra2 = extra(train_data2,test_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Stacking <a class=\"anchor\" id=\"seventh-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from matplotlib import pyplot\n",
    "import re\n",
    "def fun2(train, test):\n",
    "    # define dataset\n",
    "  \n",
    "    # Extract the ids\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    train = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    X = train\n",
    "    y = labels\n",
    "    # define the base models\n",
    "    level0 = list()  \n",
    "    level0.append(('dt', DecisionTreeClassifier()))\n",
    "    weights = {0:0.9122, 1:0.0878}\n",
    "    level0.append(('lr', LogisticRegression(C = 0.0001, class_weight=weights)))\n",
    "    level0.append(('sth',BalancedRandomForestClassifier()))\n",
    "    # define meta learner model\n",
    "   \n",
    "    level1 =BalancedRandomForestClassifier()\n",
    "    # define the stacking ensemble\n",
    "    model = StackingClassifier(estimators=level0, final_estimator=level1, cv=2)\n",
    "    # fit the model on all available data\n",
    "    model.fit(X, y)\n",
    "    # make a prediction for one example\n",
    "    model_pred = model.predict_proba(test)[:,1]\n",
    "    tescik = model.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    cm = confusion_matrix(test_labels, tescik)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, model_pred))\n",
    "    print(\"F1 score \", f1)\n",
    "    \n",
    "    return roc_auc_score(test_labels, model_pred), model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "StackingScore1 = fun2(train_data1,test_data1)\n",
    "StackingScore2 = fun2(train_data2,test_data2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Boosting <a class=\"anchor\" id=\"eigth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 LightGBM <a class=\"anchor\" id=\"nineth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "def PlainLightGBM(train,test):\n",
    "    \n",
    "     # Extract the ids\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "   \n",
    "    model = lgb.LGBMClassifier(class_weight ='balanced' )\n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    train = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    test = test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=1)\n",
    "    n_scores = cross_val_score(model, train, labels, scoring='roc_auc', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "    model.fit(train,labels)\n",
    "    model_pred =  model.predict_proba(test)[:, 1]\n",
    "   \n",
    "    tescik =  model.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "     \n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, model_pred))\n",
    "    print('F1 score: %f' % f1)\n",
    "    print('Average F1 score: %f' % mean(n_scores))\n",
    "    cm = confusion_matrix(test_labels, tescik)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()\n",
    "    return roc_auc_score(test_labels, model_pred), model_pred\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBMScore1=PlainLightGBM(train_data1,test_data1)\n",
    "LGBMScore2=PlainLightGBM(train_data2,test_data2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 Gradient Boosting  <a class=\"anchor\" id=\"tenth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# make predictions using gradient boosting for classification\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import re\n",
    "\n",
    "def myBooster(train,test):\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    train = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    # define dataset\n",
    "    X = train\n",
    "    y = labels\n",
    "    # define the model\n",
    "    model = GradientBoostingClassifier()\n",
    "    cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=1)\n",
    "    n_scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    model.fit(X, y)\n",
    "    # make a prediction for one example\n",
    "    model_pred = model.predict_proba(test)[:,1]\n",
    "    tescik = model.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    cm = confusion_matrix(test_labels, tescik)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, model_pred))\n",
    "    print('F1 score: %f' % f1)\n",
    "    print('Average F1 score: %f' % mean(n_scores))\n",
    "    return roc_auc_score(test_labels, model_pred), model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GBScore1=myBooster(train_data1,test_data1)\n",
    "# GBScore2=myBooster(train_data2,test_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.3 Histogram Gradient Boosting  <a class=\"anchor\" id=\"11-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "def myHistBooster(train, test):\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    train = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    # define dataset\n",
    "    X = train\n",
    "    y = labels\n",
    "    model = HistGradientBoostingClassifier()\n",
    "    cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n",
    "    n_scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1, error_score='raise')\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    model_pred = model.predict_proba(test)[:,1]\n",
    "    tescik = model.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    cm = confusion_matrix(test_labels, tescik)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, model_pred))\n",
    "    print('F1 score: %f' % f1)\n",
    "    print('Average F1 score: %f' % mean(n_scores))\n",
    "    return roc_auc_score(test_labels, model_pred), model_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HB1Score1 = myHistBooster(train_data1,test_data1)\n",
    "HB1Score2 = myHistBooster(train_data2,test_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Bagging <a class=\"anchor\" id=\"12-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import model_selection \n",
    "from sklearn.ensemble import BaggingClassifier \n",
    "def bagging(train, test):\n",
    "    train_ids = train['SK_ID_CURR']\n",
    "    test_ids = test['SK_ID_CURR']\n",
    "    \n",
    "    # Extract the labels for training\n",
    "    labels = train['TARGET']\n",
    "    test_labels = test['TARGET']\n",
    "    # Remove the ids and target\n",
    "    train = train.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    test= test.drop(columns = ['TARGET','SK_ID_CURR'])\n",
    "    train = train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    \n",
    "    # define dataset\n",
    "    X = train\n",
    "    y = labels\n",
    "    seed = 1\n",
    "   \n",
    "    # initialize the base classifier \n",
    "    base_cls = RandomForestClassifier(n_estimators = 100, n_jobs = -1)\n",
    "    base_cls1 = BalancedRandomForestClassifier()\n",
    "    weights = {0:0.9122, 1:0.0878}\n",
    "    base_cls2 = LogisticRegression(C = 0.0001, class_weight=weights)\n",
    "  \n",
    "    # no. of base classifier \n",
    "    num_trees = 100\n",
    "  \n",
    "    # bagging classifier \n",
    "    model = BaggingClassifier(base_estimator = base_cls2, \n",
    "                          random_state = seed,\n",
    "                             n_jobs=-1) \n",
    "    print(\"Fiting\")\n",
    "    model.fit(X,y)\n",
    "    model_pred = model.predict_proba(test)[:,1]\n",
    "    tescik = model.predict(test)\n",
    "    f1 = f1_score(test_labels, tescik)\n",
    "    cm = confusion_matrix(test_labels, tescik)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    ax.imshow(cm)\n",
    "    ax.grid(False)\n",
    "    ax.xaxis.set(ticks=(0, 1), ticklabels=('Predicted 0s', 'Predicted 1s'))\n",
    "    ax.yaxis.set(ticks=(0, 1), ticklabels=('Actual 0s', 'Actual 1s'))\n",
    "    ax.set_ylim(1.5, -0.5)\n",
    "    for i in range(2):\n",
    "        for j in range(2):\n",
    "            ax.text(j, i, cm[i, j], ha='center', va='center', color='red')\n",
    "    plt.show()\n",
    "    \n",
    "    print('Train/Test split results:')\n",
    "    print(\"ROC\",  roc_auc_score(test_labels, model_pred)), model_pred\n",
    "    print('F1 score: %f' % f1)\n",
    "    \n",
    "    return roc_auc_score(test_labels, model_pred), model_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BagginScore1=bagging(train_data1,test_data1)\n",
    "BagginScore2=bagging(train_data2,test_data2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Plotting <a class=\"anchor\" id=\"13-bullet\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "def plotingROC(test_labels,test_labels1):\n",
    "    fpr, tpr, thresh = roc_curve(test_labels, DTScore2[1], pos_label=1)\n",
    "    fpr1, tpr1, thresh1 = roc_curve(test_labels, KNNScore2[1], pos_label=1)\n",
    "    fpr2, tpr2, thresh2 = roc_curve(test_labels1, NBScore1[1], pos_label=1)\n",
    "    fpr3, tpr3, thresh3 = roc_curve(test_labels, LRScore2[1], pos_label=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    random_probs = [0 for i in range(len(test_labels))]\n",
    "    p_fpr, p_tpr,_  = roc_curve(test_labels, random_probs, pos_label=1)\n",
    "    plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "    plt.plot(fpr, tpr, linestyle='--', color='orange', label='Decision Tree')\n",
    "    plt.plot(fpr1, tpr1, linestyle='--', color='red', label='KNN')\n",
    "    plt.plot(fpr2, tpr2, linestyle='--', color='green', label='Naive Bayes')\n",
    "    plt.plot(fpr3, tpr3, linestyle='--', color='black', label='Logistic Regression')\n",
    "    \n",
    "  \n",
    "    plt.title('ROC curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive rate')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show();\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotingROC1(test_labels,test_labels1):\n",
    "    fpr, tpr, thresh = roc_curve(test_labels, LGBMScore2[1], pos_label=1)\n",
    "#     fpr1, tpr1, thresh1 = roc_curve(test_labels1, GBScore2[1], pos_label=1)\n",
    "#     fpr2, tpr2, thresh2 = roc_curve(test_labels, BagginScore2[1], pos_label=1)\n",
    "    fpr3, tpr3, thresh3 = roc_curve(test_labels, HB1Score2[1], pos_label=1)\n",
    "    fpr4, tpr4, thresh4 = roc_curve(test_labels, StackingScore2[1], pos_label=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    random_probs = [0 for i in range(len(test_labels))]\n",
    "    p_fpr, p_tpr,_  = roc_curve(test_labels, random_probs, pos_label=1)\n",
    "    plt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n",
    "    plt.plot(fpr, tpr, linestyle='--', color='orange', label='LightGBM')\n",
    "#     plt.plot(fpr1, tpr1, linestyle='--', color='red', label='Gradient Boosting')\n",
    "#     plt.plot(fpr2, tpr2, linestyle='--', color='green', label='Bagging')\n",
    "    plt.plot(fpr3, tpr3, linestyle='--', color='black', label='Histogram Boosting')\n",
    "    plt.plot(fpr4, tpr4, linestyle='--', color='pink', label='Stacking')\n",
    "    \n",
    "   \n",
    "    plt.title('ROC curve')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive rate')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show();\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotingROC(test_data2[\"TARGET\"],test_data1[\"TARGET\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plotingROC1(test_data2[\"TARGET\"],test_data1[\"TARGET\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"LGBM: %f\" % LGBMScore2[0]);\n",
    "print(\"Gradient: %f\" % GBScore1[0]);\n",
    "print(\"Bagging: %f\" % BagginScore2[0]);\n",
    "print(\"Histogram: %f\" % HB1Score2[0]);\n",
    "print(\"Stacking: %f\" % StackingScore2[0]);\n",
    "\n",
    "\n",
    "print(\"Decision Trees: %f\" % DTScore2[0]);\n",
    "print(\"KNN: %f\" % KNNScore2[0]);\n",
    "print(\"Logisitc Regression: %f\" % LRScore2[0]);\n",
    "print(\"NaiveBayes: %f\" % NBScore1[0]);\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
